{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data = pd.read_csv('creditcard.csv')\n",
    "recession_data = pd.read_csv('africa_recession.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_label = cc_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cc_data['Class']\n",
    "del cc_data['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pop</th>\n",
       "      <th>emp</th>\n",
       "      <th>emp_to_pop_ratio</th>\n",
       "      <th>hc</th>\n",
       "      <th>ccon</th>\n",
       "      <th>cda</th>\n",
       "      <th>cn</th>\n",
       "      <th>ck</th>\n",
       "      <th>ctfp</th>\n",
       "      <th>cwtfp</th>\n",
       "      <th>...</th>\n",
       "      <th>agriculture</th>\n",
       "      <th>fish</th>\n",
       "      <th>total_change</th>\n",
       "      <th>excl_energy_change</th>\n",
       "      <th>energy_change</th>\n",
       "      <th>metals_minerals_change</th>\n",
       "      <th>forestry_change</th>\n",
       "      <th>agriculture_change</th>\n",
       "      <th>fish_change</th>\n",
       "      <th>growthbucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.849621</td>\n",
       "      <td>6.914298</td>\n",
       "      <td>0.239667</td>\n",
       "      <td>1.547767</td>\n",
       "      <td>99010.171880</td>\n",
       "      <td>131991.375000</td>\n",
       "      <td>4.265433e+05</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>0.648979</td>\n",
       "      <td>0.626111</td>\n",
       "      <td>...</td>\n",
       "      <td>149.37</td>\n",
       "      <td>876.46</td>\n",
       "      <td>0.264843</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.717188</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.728313</td>\n",
       "      <td>13.623806</td>\n",
       "      <td>0.297929</td>\n",
       "      <td>2.086037</td>\n",
       "      <td>316862.656300</td>\n",
       "      <td>374842.343800</td>\n",
       "      <td>8.046022e+05</td>\n",
       "      <td>0.019658</td>\n",
       "      <td>0.719390</td>\n",
       "      <td>0.657543</td>\n",
       "      <td>...</td>\n",
       "      <td>149.37</td>\n",
       "      <td>876.46</td>\n",
       "      <td>0.264843</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.717188</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.262043</td>\n",
       "      <td>13.323813</td>\n",
       "      <td>0.400571</td>\n",
       "      <td>1.498783</td>\n",
       "      <td>29762.876950</td>\n",
       "      <td>36584.992190</td>\n",
       "      <td>6.707259e+04</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.308652</td>\n",
       "      <td>0.292607</td>\n",
       "      <td>...</td>\n",
       "      <td>149.37</td>\n",
       "      <td>876.46</td>\n",
       "      <td>0.264843</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.717188</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.025703</td>\n",
       "      <td>3.869000</td>\n",
       "      <td>0.482076</td>\n",
       "      <td>1.356151</td>\n",
       "      <td>6488.692871</td>\n",
       "      <td>7006.969727</td>\n",
       "      <td>5.790397e+03</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.205865</td>\n",
       "      <td>0.234643</td>\n",
       "      <td>...</td>\n",
       "      <td>149.37</td>\n",
       "      <td>876.46</td>\n",
       "      <td>0.264843</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.717188</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.061468</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.228928</td>\n",
       "      <td>1.676330</td>\n",
       "      <td>7620.187500</td>\n",
       "      <td>8419.264648</td>\n",
       "      <td>1.690724e+04</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.959765</td>\n",
       "      <td>0.993985</td>\n",
       "      <td>...</td>\n",
       "      <td>149.37</td>\n",
       "      <td>876.46</td>\n",
       "      <td>0.264843</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.717188</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>0.073986</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>12.208407</td>\n",
       "      <td>6.215000</td>\n",
       "      <td>0.509075</td>\n",
       "      <td>1.837961</td>\n",
       "      <td>21853.750000</td>\n",
       "      <td>26727.062500</td>\n",
       "      <td>5.534759e+04</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.234562</td>\n",
       "      <td>0.248461</td>\n",
       "      <td>...</td>\n",
       "      <td>220.62</td>\n",
       "      <td>1262.71</td>\n",
       "      <td>0.162721</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.239561</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>29.784193</td>\n",
       "      <td>15.389316</td>\n",
       "      <td>0.516694</td>\n",
       "      <td>1.467321</td>\n",
       "      <td>94663.101560</td>\n",
       "      <td>138279.359400</td>\n",
       "      <td>1.080252e+06</td>\n",
       "      <td>0.016209</td>\n",
       "      <td>0.337927</td>\n",
       "      <td>0.287038</td>\n",
       "      <td>...</td>\n",
       "      <td>220.62</td>\n",
       "      <td>1262.71</td>\n",
       "      <td>0.162721</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.239561</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>55.797787</td>\n",
       "      <td>25.301973</td>\n",
       "      <td>0.453458</td>\n",
       "      <td>1.689919</td>\n",
       "      <td>102405.859400</td>\n",
       "      <td>137377.593800</td>\n",
       "      <td>7.739462e+05</td>\n",
       "      <td>0.009984</td>\n",
       "      <td>0.259968</td>\n",
       "      <td>0.248847</td>\n",
       "      <td>...</td>\n",
       "      <td>220.62</td>\n",
       "      <td>1262.71</td>\n",
       "      <td>0.162721</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.239561</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>15.850567</td>\n",
       "      <td>5.277823</td>\n",
       "      <td>0.332974</td>\n",
       "      <td>1.579579</td>\n",
       "      <td>42087.269530</td>\n",
       "      <td>55202.140630</td>\n",
       "      <td>1.829390e+05</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.470906</td>\n",
       "      <td>0.491789</td>\n",
       "      <td>...</td>\n",
       "      <td>220.62</td>\n",
       "      <td>1262.71</td>\n",
       "      <td>0.162721</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.239561</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>7.797694</td>\n",
       "      <td>3.468000</td>\n",
       "      <td>0.444747</td>\n",
       "      <td>1.790903</td>\n",
       "      <td>11765.336910</td>\n",
       "      <td>13920.992190</td>\n",
       "      <td>5.599717e+04</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.175976</td>\n",
       "      <td>0.188004</td>\n",
       "      <td>...</td>\n",
       "      <td>220.62</td>\n",
       "      <td>1262.71</td>\n",
       "      <td>0.162721</td>\n",
       "      <td>0.105791</td>\n",
       "      <td>0.239561</td>\n",
       "      <td>0.078946</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.026477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pop        emp  emp_to_pop_ratio        hc           ccon  \\\n",
       "0    28.849621   6.914298          0.239667  1.547767   99010.171880   \n",
       "1    45.728313  13.623806          0.297929  2.086037  316862.656300   \n",
       "2    33.262043  13.323813          0.400571  1.498783   29762.876950   \n",
       "3     8.025703   3.869000          0.482076  1.356151    6488.692871   \n",
       "4     1.061468   0.243000          0.228928  1.676330    7620.187500   \n",
       "..         ...        ...               ...       ...            ...   \n",
       "481  12.208407   6.215000          0.509075  1.837961   21853.750000   \n",
       "482  29.784193  15.389316          0.516694  1.467321   94663.101560   \n",
       "483  55.797787  25.301973          0.453458  1.689919  102405.859400   \n",
       "484  15.850567   5.277823          0.332974  1.579579   42087.269530   \n",
       "485   7.797694   3.468000          0.444747  1.790903   11765.336910   \n",
       "\n",
       "               cda            cn        ck      ctfp     cwtfp  ...  \\\n",
       "0    131991.375000  4.265433e+05  0.009374  0.648979  0.626111  ...   \n",
       "1    374842.343800  8.046022e+05  0.019658  0.719390  0.657543  ...   \n",
       "2     36584.992190  6.707259e+04  0.001670  0.308652  0.292607  ...   \n",
       "3      7006.969727  5.790397e+03  0.000184  0.205865  0.234643  ...   \n",
       "4      8419.264648  1.690724e+04  0.000431  0.959765  0.993985  ...   \n",
       "..             ...           ...       ...       ...       ...  ...   \n",
       "481   26727.062500  5.534759e+04  0.000896  0.234562  0.248461  ...   \n",
       "482  138279.359400  1.080252e+06  0.016209  0.337927  0.287038  ...   \n",
       "483  137377.593800  7.739462e+05  0.009984  0.259968  0.248847  ...   \n",
       "484   55202.140630  1.829390e+05  0.002542  0.470906  0.491789  ...   \n",
       "485   13920.992190  5.599717e+04  0.000779  0.175976  0.188004  ...   \n",
       "\n",
       "     agriculture     fish  total_change  excl_energy_change  energy_change  \\\n",
       "0         149.37   876.46      0.264843            0.012115       0.717188   \n",
       "1         149.37   876.46      0.264843            0.012115       0.717188   \n",
       "2         149.37   876.46      0.264843            0.012115       0.717188   \n",
       "3         149.37   876.46      0.264843            0.012115       0.717188   \n",
       "4         149.37   876.46      0.264843            0.012115       0.717188   \n",
       "..           ...      ...           ...                 ...            ...   \n",
       "481       220.62  1262.71      0.162721            0.105791       0.239561   \n",
       "482       220.62  1262.71      0.162721            0.105791       0.239561   \n",
       "483       220.62  1262.71      0.162721            0.105791       0.239561   \n",
       "484       220.62  1262.71      0.162721            0.105791       0.239561   \n",
       "485       220.62  1262.71      0.162721            0.105791       0.239561   \n",
       "\n",
       "     metals_minerals_change  forestry_change  agriculture_change  fish_change  \\\n",
       "0                  0.099935        -0.061177            0.073986     0.007576   \n",
       "1                  0.099935        -0.061177            0.073986     0.007576   \n",
       "2                  0.099935        -0.061177            0.073986     0.007576   \n",
       "3                  0.099935        -0.061177            0.073986     0.007576   \n",
       "4                  0.099935        -0.061177            0.073986     0.007576   \n",
       "..                      ...              ...                 ...          ...   \n",
       "481                0.078946         0.198187            0.073211     0.026477   \n",
       "482                0.078946         0.198187            0.073211     0.026477   \n",
       "483                0.078946         0.198187            0.073211     0.026477   \n",
       "484                0.078946         0.198187            0.073211     0.026477   \n",
       "485                0.078946         0.198187            0.073211     0.026477   \n",
       "\n",
       "     growthbucket  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "..            ...  \n",
       "481             0  \n",
       "482             1  \n",
       "483             0  \n",
       "484             0  \n",
       "485             0  \n",
       "\n",
       "[486 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recession_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recession_label = recession_data['growthbucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del recession_data['growthbucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components = 15)\n",
    "principal_components = pca.fit(recession_data)\n",
    "x = list(range(1,15))\n",
    "y = [sum(pca.explained_variance_ratio_[:n]) for n in x]\n",
    "plt.plot(x,y, 'ko')\n",
    "plt.plot(x,y,'r-')\n",
    "plt.xlabel('dimensions')\n",
    "plt.ylabel('explained variance ratio')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.PCA(n_components = 3)\n",
    "principal_components = pca.fit_transform(recession_data)\n",
    "recession_pca = pd.DataFrame(data = principal_components, columns = ['principal component 1','principal component 2', 'principal component 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component 1</th>\n",
       "      <th>principal component 2</th>\n",
       "      <th>principal component 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.232731e+05</td>\n",
       "      <td>82909.714223</td>\n",
       "      <td>-139174.986299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.493806e+06</td>\n",
       "      <td>438167.211793</td>\n",
       "      <td>-249557.407441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.319926e+05</td>\n",
       "      <td>40363.184785</td>\n",
       "      <td>-9490.551161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.962167e+05</td>\n",
       "      <td>-9001.273880</td>\n",
       "      <td>19005.620294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.680777e+05</td>\n",
       "      <td>-3414.865523</td>\n",
       "      <td>3435.511882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>-3.247187e+05</td>\n",
       "      <td>-13554.787486</td>\n",
       "      <td>31466.095241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>1.000699e+06</td>\n",
       "      <td>-299463.737922</td>\n",
       "      <td>-192737.319311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>6.265769e+05</td>\n",
       "      <td>-189593.500267</td>\n",
       "      <td>-76714.363207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>-1.459176e+05</td>\n",
       "      <td>-35730.022984</td>\n",
       "      <td>20915.643952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-3.315248e+05</td>\n",
       "      <td>-22926.010826</td>\n",
       "      <td>10704.934038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     principal component 1  principal component 2  principal component 3\n",
       "0             4.232731e+05           82909.714223         -139174.986299\n",
       "1             1.493806e+06          438167.211793         -249557.407441\n",
       "2            -2.319926e+05           40363.184785           -9490.551161\n",
       "3            -3.962167e+05           -9001.273880           19005.620294\n",
       "4            -3.680777e+05           -3414.865523            3435.511882\n",
       "..                     ...                    ...                    ...\n",
       "481          -3.247187e+05          -13554.787486           31466.095241\n",
       "482           1.000699e+06         -299463.737922         -192737.319311\n",
       "483           6.265769e+05         -189593.500267          -76714.363207\n",
       "484          -1.459176e+05          -35730.022984           20915.643952\n",
       "485          -3.315248e+05          -22926.010826           10704.934038\n",
       "\n",
       "[486 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recession_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card class imbalance\n",
      "{0: 284315, 1: 492}\n",
      "Recession Class Imbalance\n",
      "{0: 448, 1: 38}\n"
     ]
    }
   ],
   "source": [
    "### see the data balance in both labels now\n",
    "unique, counts = np.unique(cc_label, return_counts = True)\n",
    "print('Credit Card class imbalance')\n",
    "print(dict(zip(unique,counts)))\n",
    "unique, counts = np.unique(recession_label, return_counts = True)\n",
    "print('Recession Class Imbalance')\n",
    "print(dict(zip(unique,counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 284315, 1: 284315})\n",
      "Counter({0: 448, 1: 448})\n"
     ]
    }
   ],
   "source": [
    "### random over sampling\n",
    "ros = RandomOverSampler()\n",
    "x_cc_ros, y_cc_ros = ros.fit_resample(cc_data, cc_label)\n",
    "x_recession_ros, y_recession_ros = ros.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_ros))\n",
    "print(Counter(y_recession_ros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 492, 1: 492})\n",
      "Counter({0: 38, 1: 38})\n"
     ]
    }
   ],
   "source": [
    "### random undersampling\n",
    "rus = RandomUnderSampler()\n",
    "x_cc_rus, y_cc_rus = rus.fit_resample(cc_data, cc_label)\n",
    "x_recession_rus, y_recession_rus = rus.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_rus))\n",
    "print(Counter(y_recession_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 492, 1: 492})\n",
      "Counter({0: 38, 1: 38})\n"
     ]
    }
   ],
   "source": [
    "### Centroid based undersampling\n",
    "cc = ClusterCentroids()\n",
    "x_cc_res, y_cc_res = cc.fit_resample(cc_data,cc_label)\n",
    "x_recession_res, y_recession_res = cc.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_res))\n",
    "print(Counter(y_recession_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 492, 1: 492})\n",
      "Counter({0: 38, 1: 38})\n"
     ]
    }
   ],
   "source": [
    "### Near Miss undersampling\n",
    "nm = NearMiss()\n",
    "x_cc_nm, y_cc_nm = nm.fit_resample(cc_data, cc_label)\n",
    "x_recession_nm, y_recession_nm = nm.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_nm))\n",
    "print(Counter(y_recession_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 284315, 1: 284315})\n",
      "Counter({0: 448, 1: 448})\n"
     ]
    }
   ],
   "source": [
    "### SMOTE\n",
    "smt = SMOTE()\n",
    "x_cc_smt, y_cc_smt = smt.fit_resample(cc_data,cc_label)\n",
    "x_recession_smt, y_recession_smt = smt.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_smt))\n",
    "print(Counter(y_recession_smt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 284315, 1: 284240})\n",
      "Counter({0: 448, 1: 439})\n"
     ]
    }
   ],
   "source": [
    "### ADASYN\n",
    "ada = ADASYN()\n",
    "x_cc_ada, y_cc_ada = ada.fit_resample(cc_data, cc_label)\n",
    "x_recession_ada, y_recession_ada = ada.fit_resample(recession_pca, recession_label)\n",
    "print(Counter(y_cc_ada))\n",
    "print(Counter(y_recession_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are models implemented with raw unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(cc_data, cc_label, test_size = 0.2)\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(recession_pca, recession_label, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56854     9]\n",
      " [   42    57]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56863\n",
      "           1       0.86      0.58      0.69        99\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.93      0.79      0.85     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 100000)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21 70]\n",
      " [ 0  7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.38        91\n",
      "           1       0.09      1.00      0.17         7\n",
      "\n",
      "    accuracy                           0.29        98\n",
      "   macro avg       0.55      0.62      0.27        98\n",
      "weighted avg       0.94      0.29      0.36        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter = 100000)\n",
    "logreg.fit(x1_train, y1_train)\n",
    "y1_pred = logreg.predict(x1_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y1_test, y1_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56839    24]\n",
      " [   25    74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56863\n",
      "           1       0.76      0.75      0.75        99\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.88      0.87      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[88  3]\n",
      " [ 6  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95        91\n",
      "           1       0.25      0.14      0.18         7\n",
      "\n",
      "    accuracy                           0.91        98\n",
      "   macro avg       0.59      0.55      0.57        98\n",
      "weighted avg       0.89      0.91      0.90        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(x1_train, y1_train)\n",
    "y1_pred = clf.predict(x1_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y1_test, y1_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56860     3]\n",
      " [   22    77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56863\n",
      "           1       0.96      0.78      0.86        99\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.98      0.89      0.93     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[91  0]\n",
      " [ 7  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96        91\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.93        98\n",
      "   macro avg       0.46      0.50      0.48        98\n",
      "weighted avg       0.86      0.93      0.89        98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anson\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\anson\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\anson\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "rf.fit(x1_train, y1_train)\n",
    "y1_pred = rf.predict(x1_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y1_test, y1_pred)\n",
    "print(confusion_matrix)\n",
    "print(classification_report(y1_test, y1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7121/7121 [==============================] - 5s 677us/step - loss: 0.0461 - accuracy: 0.9919\n",
      "Epoch 2/15\n",
      "7121/7121 [==============================] - 5s 691us/step - loss: 0.0050 - accuracy: 0.9992\n",
      "Epoch 3/15\n",
      "7121/7121 [==============================] - 5s 688us/step - loss: 0.0047 - accuracy: 0.9993\n",
      "Epoch 4/15\n",
      "7121/7121 [==============================] - 5s 689us/step - loss: 0.0043 - accuracy: 0.9992\n",
      "Epoch 5/15\n",
      "7121/7121 [==============================] - 5s 721us/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 6/15\n",
      "7121/7121 [==============================] - 5s 745us/step - loss: 0.0042 - accuracy: 0.9993\n",
      "Epoch 7/15\n",
      "7121/7121 [==============================] - 5s 727us/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 8/15\n",
      "7121/7121 [==============================] - 5s 725us/step - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 9/15\n",
      "7121/7121 [==============================] - 5s 719us/step - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 10/15\n",
      "7121/7121 [==============================] - 5s 740us/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 11/15\n",
      "7121/7121 [==============================] - 5s 724us/step - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 12/15\n",
      "7121/7121 [==============================] - 5s 726us/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 13/15\n",
      "7121/7121 [==============================] - 5s 729us/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 14/15\n",
      "7121/7121 [==============================] - 5s 728us/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 15/15\n",
      "7121/7121 [==============================] - 5s 733us/step - loss: 0.0024 - accuracy: 0.9995\n",
      "1781/1781 - 1s - loss: 0.0035 - accuracy: 0.9994\n",
      "[[56856     7]\n",
      " [   26    73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56863\n",
      "           1       0.91      0.74      0.82        99\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.87      0.91     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(20, activation = 'relu'),\n",
    "    Dense(5, activation = 'relu'),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=15)\n",
    "test_loss, test_acc = model.evaluate(x_test,y_test,verbose = 2)\n",
    "y_pred = model.predict(x_test)\n",
    "preds_index = np.argmax(y_pred, axis = 1)\n",
    "print(confusion_matrix(y_test,preds_index))\n",
    "print(classification_report(y_test, preds_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 49158.6336 - accuracy: 0.0968\n",
      "Epoch 2/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 44718.3228 - accuracy: 0.6267\n",
      "Epoch 3/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 21095.5587 - accuracy: 0.7036\n",
      "Epoch 4/15\n",
      "13/13 [==============================] - 0s 666us/step - loss: 11677.1631 - accuracy: 0.7586\n",
      "Epoch 5/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 5384.9889 - accuracy: 0.7532\n",
      "Epoch 6/15\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2773.7326 - accuracy: 0.7332\n",
      "Epoch 7/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 1513.9830 - accuracy: 0.7700\n",
      "Epoch 8/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 1756.0788 - accuracy: 0.7936\n",
      "Epoch 9/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 1718.3090 - accuracy: 0.7885\n",
      "Epoch 10/15\n",
      "13/13 [==============================] - 0s 666us/step - loss: 937.6355 - accuracy: 0.8002\n",
      "Epoch 11/15\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1028.2105 - accuracy: 0.7899\n",
      "Epoch 12/15\n",
      "13/13 [==============================] - 0s 337us/step - loss: 1272.0362 - accuracy: 0.7837\n",
      "Epoch 13/15\n",
      "13/13 [==============================] - 0s 667us/step - loss: 910.9948 - accuracy: 0.8085\n",
      "Epoch 14/15\n",
      "13/13 [==============================] - 0s 0s/step - loss: 710.3116 - accuracy: 0.7620\n",
      "Epoch 15/15\n",
      "13/13 [==============================] - 0s 664us/step - loss: 714.7537 - accuracy: 0.7966\n",
      "4/4 - 0s - loss: 271.2826 - accuracy: 0.8469\n",
      "[[83  8]\n",
      " [ 7  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92        91\n",
      "           1       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.85        98\n",
      "   macro avg       0.46      0.46      0.46        98\n",
      "weighted avg       0.86      0.85      0.85        98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(20, activation = 'relu'),\n",
    "    Dense(5, activation = 'relu'),\n",
    "    Dense(2, activation = 'softmax')\n",
    "])\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x1_train, y1_train, epochs=15)\n",
    "test_loss, test_acc = model.evaluate(x1_test,y1_test,verbose = 2)\n",
    "y1_pred = model.predict(x1_test)\n",
    "preds_index = np.argmax(y1_pred, axis = 1)\n",
    "print(confusion_matrix(y1_test,preds_index))\n",
    "print(classification_report(y1_test, preds_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run balanced datasets through the same models now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_x = [x_cc_ros, x_recession_ros, x_cc_rus, x_recession_rus, x_cc_res, x_recession_res, x_cc_nm, x_recession_nm, x_cc_smt,\n",
    "             x_recession_smt, x_cc_ada, x_recession_ada]\n",
    "all_data_y = [y_cc_ros, y_recession_ros, y_cc_rus, y_recession_rus, y_cc_res, y_recession_res, y_cc_nm, y_recession_nm,\n",
    "             y_cc_smt, y_recession_smt, y_cc_ada, y_recession_ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55593  1284]\n",
      " [ 4471 52378]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     56877\n",
      "           1       0.98      0.92      0.95     56849\n",
      "\n",
      "    accuracy                           0.95    113726\n",
      "   macro avg       0.95      0.95      0.95    113726\n",
      "weighted avg       0.95      0.95      0.95    113726\n",
      "\n",
      "[[18 68]\n",
      " [ 6 88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.21      0.33        86\n",
      "           1       0.56      0.94      0.70        94\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.66      0.57      0.52       180\n",
      "weighted avg       0.65      0.59      0.52       180\n",
      "\n",
      "[[ 82   3]\n",
      " [  8 104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94        85\n",
      "           1       0.97      0.93      0.95       112\n",
      "\n",
      "    accuracy                           0.94       197\n",
      "   macro avg       0.94      0.95      0.94       197\n",
      "weighted avg       0.95      0.94      0.94       197\n",
      "\n",
      "[[3 5]\n",
      " [1 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.38      0.50         8\n",
      "           1       0.58      0.88      0.70         8\n",
      "\n",
      "    accuracy                           0.62        16\n",
      "   macro avg       0.67      0.62      0.60        16\n",
      "weighted avg       0.67      0.62      0.60        16\n",
      "\n",
      "[[93  2]\n",
      " [10 92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94        95\n",
      "           1       0.98      0.90      0.94       102\n",
      "\n",
      "    accuracy                           0.94       197\n",
      "   macro avg       0.94      0.94      0.94       197\n",
      "weighted avg       0.94      0.94      0.94       197\n",
      "\n",
      "[[6 2]\n",
      " [2 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75         8\n",
      "           1       0.75      0.75      0.75         8\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.75      0.75      0.75        16\n",
      "weighted avg       0.75      0.75      0.75        16\n",
      "\n",
      "[[ 93   0]\n",
      " [  0 104]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        93\n",
      "           1       1.00      1.00      1.00       104\n",
      "\n",
      "    accuracy                           1.00       197\n",
      "   macro avg       1.00      1.00      1.00       197\n",
      "weighted avg       1.00      1.00      1.00       197\n",
      "\n",
      "[[5 0]\n",
      " [4 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.71         5\n",
      "           1       1.00      0.64      0.78        11\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.78      0.82      0.75        16\n",
      "weighted avg       0.86      0.75      0.76        16\n",
      "\n",
      "[[55781   928]\n",
      " [ 3599 53418]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     56709\n",
      "           1       0.98      0.94      0.96     57017\n",
      "\n",
      "    accuracy                           0.96    113726\n",
      "   macro avg       0.96      0.96      0.96    113726\n",
      "weighted avg       0.96      0.96      0.96    113726\n",
      "\n",
      "[[13 76]\n",
      " [ 2 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.15      0.25        89\n",
      "           1       0.54      0.98      0.70        91\n",
      "\n",
      "    accuracy                           0.57       180\n",
      "   macro avg       0.70      0.56      0.47       180\n",
      "weighted avg       0.70      0.57      0.48       180\n",
      "\n",
      "[[53692  2954]\n",
      " [ 6436 50629]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     56646\n",
      "           1       0.94      0.89      0.92     57065\n",
      "\n",
      "    accuracy                           0.92    113711\n",
      "   macro avg       0.92      0.92      0.92    113711\n",
      "weighted avg       0.92      0.92      0.92    113711\n",
      "\n",
      "[[21 69]\n",
      " [ 4 84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.23      0.37        90\n",
      "           1       0.55      0.95      0.70        88\n",
      "\n",
      "    accuracy                           0.59       178\n",
      "   macro avg       0.69      0.59      0.53       178\n",
      "weighted avg       0.70      0.59      0.53       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(all_data_x)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data_x[i], all_data_y[i], test_size = 0.2)\n",
    "    logreg = LogisticRegression(max_iter = 100000)\n",
    "    logreg.fit(x_train, y_train)\n",
    "    y_pred = logreg.predict(x_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56751    22]\n",
      " [    0 56953]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56773\n",
      "           1       1.00      1.00      1.00     56953\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "[[80  4]\n",
      " [ 0 96]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        84\n",
      "           1       0.96      1.00      0.98        96\n",
      "\n",
      "    accuracy                           0.98       180\n",
      "   macro avg       0.98      0.98      0.98       180\n",
      "weighted avg       0.98      0.98      0.98       180\n",
      "\n",
      "[[94 12]\n",
      " [ 9 82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       106\n",
      "           1       0.87      0.90      0.89        91\n",
      "\n",
      "    accuracy                           0.89       197\n",
      "   macro avg       0.89      0.89      0.89       197\n",
      "weighted avg       0.89      0.89      0.89       197\n",
      "\n",
      "[[8 1]\n",
      " [3 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80         9\n",
      "           1       0.80      0.57      0.67         7\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.76      0.73      0.73        16\n",
      "weighted avg       0.76      0.75      0.74        16\n",
      "\n",
      "[[90  1]\n",
      " [11 95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94        91\n",
      "           1       0.99      0.90      0.94       106\n",
      "\n",
      "    accuracy                           0.94       197\n",
      "   macro avg       0.94      0.94      0.94       197\n",
      "weighted avg       0.94      0.94      0.94       197\n",
      "\n",
      "[[8 3]\n",
      " [1 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80        11\n",
      "           1       0.57      0.80      0.67         5\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.73      0.76      0.73        16\n",
      "weighted avg       0.79      0.75      0.76        16\n",
      "\n",
      "[[103   0]\n",
      " [  4  90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       103\n",
      "           1       1.00      0.96      0.98        94\n",
      "\n",
      "    accuracy                           0.98       197\n",
      "   macro avg       0.98      0.98      0.98       197\n",
      "weighted avg       0.98      0.98      0.98       197\n",
      "\n",
      "[[3 4]\n",
      " [0 9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.43      0.60         7\n",
      "           1       0.69      1.00      0.82         9\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.85      0.71      0.71        16\n",
      "weighted avg       0.83      0.75      0.72        16\n",
      "\n",
      "[[56563   174]\n",
      " [   50 56939]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56737\n",
      "           1       1.00      1.00      1.00     56989\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "[[74 18]\n",
      " [12 76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83        92\n",
      "           1       0.81      0.86      0.84        88\n",
      "\n",
      "    accuracy                           0.83       180\n",
      "   macro avg       0.83      0.83      0.83       180\n",
      "weighted avg       0.84      0.83      0.83       180\n",
      "\n",
      "[[56704   125]\n",
      " [   44 56838]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56829\n",
      "           1       1.00      1.00      1.00     56882\n",
      "\n",
      "    accuracy                           1.00    113711\n",
      "   macro avg       1.00      1.00      1.00    113711\n",
      "weighted avg       1.00      1.00      1.00    113711\n",
      "\n",
      "[[74 17]\n",
      " [12 75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.84        91\n",
      "           1       0.82      0.86      0.84        87\n",
      "\n",
      "    accuracy                           0.84       178\n",
      "   macro avg       0.84      0.84      0.84       178\n",
      "weighted avg       0.84      0.84      0.84       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(all_data_x)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data_x[i], all_data_y[i], test_size = 0.2)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56670     3]\n",
      " [    0 57053]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56673\n",
      "           1       1.00      1.00      1.00     57053\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "[[89  5]\n",
      " [ 0 86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        94\n",
      "           1       0.95      1.00      0.97        86\n",
      "\n",
      "    accuracy                           0.97       180\n",
      "   macro avg       0.97      0.97      0.97       180\n",
      "weighted avg       0.97      0.97      0.97       180\n",
      "\n",
      "[[86  0]\n",
      " [14 97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92        86\n",
      "           1       1.00      0.87      0.93       111\n",
      "\n",
      "    accuracy                           0.93       197\n",
      "   macro avg       0.93      0.94      0.93       197\n",
      "weighted avg       0.94      0.93      0.93       197\n",
      "\n",
      "[[2 4]\n",
      " [1 9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.33      0.44         6\n",
      "           1       0.69      0.90      0.78        10\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.68      0.62      0.61        16\n",
      "weighted avg       0.68      0.69      0.66        16\n",
      "\n",
      "[[ 88   3]\n",
      " [  4 102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96        91\n",
      "           1       0.97      0.96      0.97       106\n",
      "\n",
      "    accuracy                           0.96       197\n",
      "   macro avg       0.96      0.96      0.96       197\n",
      "weighted avg       0.96      0.96      0.96       197\n",
      "\n",
      "[[7 1]\n",
      " [0 8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93         8\n",
      "           1       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.94      0.94      0.94        16\n",
      "weighted avg       0.94      0.94      0.94        16\n",
      "\n",
      "[[97  0]\n",
      " [ 1 99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        97\n",
      "           1       1.00      0.99      0.99       100\n",
      "\n",
      "    accuracy                           0.99       197\n",
      "   macro avg       0.99      0.99      0.99       197\n",
      "weighted avg       0.99      0.99      0.99       197\n",
      "\n",
      "[[5 1]\n",
      " [3 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71         6\n",
      "           1       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.75        16\n",
      "   macro avg       0.75      0.77      0.75        16\n",
      "weighted avg       0.78      0.75      0.75        16\n",
      "\n",
      "[[56828    16]\n",
      " [    0 56882]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56844\n",
      "           1       1.00      1.00      1.00     56882\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "[[73 21]\n",
      " [ 9 77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.78      0.83        94\n",
      "           1       0.79      0.90      0.84        86\n",
      "\n",
      "    accuracy                           0.83       180\n",
      "   macro avg       0.84      0.84      0.83       180\n",
      "weighted avg       0.84      0.83      0.83       180\n",
      "\n",
      "[[56714    14]\n",
      " [    0 56983]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56728\n",
      "           1       1.00      1.00      1.00     56983\n",
      "\n",
      "    accuracy                           1.00    113711\n",
      "   macro avg       1.00      1.00      1.00    113711\n",
      "weighted avg       1.00      1.00      1.00    113711\n",
      "\n",
      "[[76 16]\n",
      " [ 7 79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87        92\n",
      "           1       0.83      0.92      0.87        86\n",
      "\n",
      "    accuracy                           0.87       178\n",
      "   macro avg       0.87      0.87      0.87       178\n",
      "weighted avg       0.88      0.87      0.87       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(all_data_x)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data_x[i], all_data_y[i], test_size = 0.2)\n",
    "    rf = RandomForestClassifier(n_estimators = 100)\n",
    "    rf.fit(x_train, y_train)\n",
    "    y_pred = rf.predict(x_test)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(confusion_matrix)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14216/14216 [==============================] - 10s 691us/step - loss: 0.1850 - accuracy: 0.9481\n",
      "Epoch 2/15\n",
      "14216/14216 [==============================] - 10s 722us/step - loss: 0.0424 - accuracy: 0.9855\n",
      "Epoch 3/15\n",
      "14216/14216 [==============================] - 10s 723us/step - loss: 0.0298 - accuracy: 0.9895\n",
      "Epoch 4/15\n",
      "14216/14216 [==============================] - 10s 715us/step - loss: 0.0251 - accuracy: 0.9913\n",
      "Epoch 5/15\n",
      "14216/14216 [==============================] - 10s 722us/step - loss: 0.0202 - accuracy: 0.9937\n",
      "Epoch 6/15\n",
      "14216/14216 [==============================] - 10s 730us/step - loss: 0.0179 - accuracy: 0.9946\n",
      "Epoch 7/15\n",
      "14216/14216 [==============================] - 11s 740us/step - loss: 0.0167 - accuracy: 0.9952\n",
      "Epoch 8/15\n",
      "14216/14216 [==============================] - 11s 753us/step - loss: 0.0157 - accuracy: 0.9953\n",
      "Epoch 9/15\n",
      "14216/14216 [==============================] - 10s 738us/step - loss: 0.0129 - accuracy: 0.9964\n",
      "Epoch 10/15\n",
      "14216/14216 [==============================] - 10s 729us/step - loss: 0.0120 - accuracy: 0.9966\n",
      "Epoch 11/15\n",
      "14216/14216 [==============================] - 10s 735us/step - loss: 0.0118 - accuracy: 0.9967\n",
      "Epoch 12/15\n",
      "14216/14216 [==============================] - 10s 731us/step - loss: 0.0108 - accuracy: 0.9969\n",
      "Epoch 13/15\n",
      "14216/14216 [==============================] - 10s 735us/step - loss: 0.0104 - accuracy: 0.9972\n",
      "Epoch 14/15\n",
      "14216/14216 [==============================] - 10s 734us/step - loss: 0.0134 - accuracy: 0.9968\n",
      "Epoch 15/15\n",
      "14216/14216 [==============================] - 10s 732us/step - loss: 0.0119 - accuracy: 0.9973\n",
      "3554/3554 - 2s - loss: 0.0124 - accuracy: 0.9963\n",
      "[[56277   419]\n",
      " [    0 57030]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56696\n",
      "           1       0.99      1.00      1.00     57030\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "Epoch 1/15\n",
      "23/23 [==============================] - 0s 728us/step - loss: 6648.7999 - accuracy: 0.5265\n",
      "Epoch 2/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 1781.9104 - accuracy: 0.4654\n",
      "Epoch 3/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 353.0495 - accuracy: 0.4740\n",
      "Epoch 4/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 243.1664 - accuracy: 0.4706\n",
      "Epoch 5/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 122.0880 - accuracy: 0.4557\n",
      "Epoch 6/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 153.7745 - accuracy: 0.4492\n",
      "Epoch 7/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 108.1448 - accuracy: 0.4769\n",
      "Epoch 8/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 70.7438 - accuracy: 0.4798\n",
      "Epoch 9/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 71.8379 - accuracy: 0.5633\n",
      "Epoch 10/15\n",
      "23/23 [==============================] - 0s 363us/step - loss: 69.1169 - accuracy: 0.5714\n",
      "Epoch 11/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 66.2579 - accuracy: 0.5693\n",
      "Epoch 12/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 50.7603 - accuracy: 0.5699\n",
      "Epoch 13/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 40.5328 - accuracy: 0.5812\n",
      "Epoch 14/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 12.0539 - accuracy: 0.5441\n",
      "Epoch 15/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 7.5007 - accuracy: 0.5616\n",
      "6/6 - 0s - loss: 31.5024 - accuracy: 0.5111\n",
      "[[12 88]\n",
      " [ 0 80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.12      0.21       100\n",
      "           1       0.48      1.00      0.65        80\n",
      "\n",
      "    accuracy                           0.51       180\n",
      "   macro avg       0.74      0.56      0.43       180\n",
      "weighted avg       0.77      0.51      0.41       180\n",
      "\n",
      "Epoch 1/15\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.4143 - accuracy: 0.3092\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 0s 830us/step - loss: 0.8699 - accuracy: 0.4640\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6464 - accuracy: 0.7194\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.5774 - accuracy: 0.7483\n",
      "Epoch 5/15\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.4409 - accuracy: 0.7945\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.4939 - accuracy: 0.8180\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.3609 - accuracy: 0.8620\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.3068 - accuracy: 0.8883\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.2572 - accuracy: 0.9277\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.2611 - accuracy: 0.9210\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.2477 - accuracy: 0.9288\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.2034 - accuracy: 0.9290\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.2018 - accuracy: 0.9339\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 0s 703us/step - loss: 0.1995 - accuracy: 0.9359\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.2470 - accuracy: 0.9022\n",
      "7/7 - 0s - loss: 0.2401 - accuracy: 0.9188\n",
      "[[90  9]\n",
      " [ 7 91]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92        99\n",
      "           1       0.91      0.93      0.92        98\n",
      "\n",
      "    accuracy                           0.92       197\n",
      "   macro avg       0.92      0.92      0.92       197\n",
      "weighted avg       0.92      0.92      0.92       197\n",
      "\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 27925.9421 - accuracy: 0.4903\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 28841.8607 - accuracy: 0.4694\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 23185.5065 - accuracy: 0.5215\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 22891.5247 - accuracy: 0.4785\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 19230.6217 - accuracy: 0.5215\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 18485.7409 - accuracy: 0.5431\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 18271.8691 - accuracy: 0.5222\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 16961.4154 - accuracy: 0.5118\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 14332.9521 - accuracy: 0.6069\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 16659.9876 - accuracy: 0.5444\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 15034.5169 - accuracy: 0.5444\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 15545.0488 - accuracy: 0.5451\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 13449.5296 - accuracy: 0.5660\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 12710.2865 - accuracy: 0.5764\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 13176.9674 - accuracy: 0.5556\n",
      "1/1 - 0s - loss: 16321.7842 - accuracy: 0.5000\n",
      "[[2 7]\n",
      " [1 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.22      0.33         9\n",
      "           1       0.46      0.86      0.60         7\n",
      "\n",
      "    accuracy                           0.50        16\n",
      "   macro avg       0.56      0.54      0.47        16\n",
      "weighted avg       0.58      0.50      0.45        16\n",
      "\n",
      "Epoch 1/15\n",
      "25/25 [==============================] - 1s 1ms/step - loss: 13.7017 - accuracy: 0.6079\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 0s 668us/step - loss: 0.4798 - accuracy: 0.8215\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.4788 - accuracy: 0.8286\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.4549 - accuracy: 0.8352\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 667us/step - loss: 0.4220 - accuracy: 0.8595\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.4233 - accuracy: 0.8556\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.4087 - accuracy: 0.8603\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.3843 - accuracy: 0.8803\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.3820 - accuracy: 0.8836\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.3636 - accuracy: 0.8821\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 0s 668us/step - loss: 0.3690 - accuracy: 0.8934\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.3492 - accuracy: 0.8926\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.3330 - accuracy: 0.9064\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 0s 668us/step - loss: 0.3448 - accuracy: 0.8947\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.3126 - accuracy: 0.9228\n",
      "7/7 - 0s - loss: 0.5219 - accuracy: 0.8680\n",
      "[[94  7]\n",
      " [19 77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       101\n",
      "           1       0.92      0.80      0.86        96\n",
      "\n",
      "    accuracy                           0.87       197\n",
      "   macro avg       0.87      0.87      0.87       197\n",
      "weighted avg       0.87      0.87      0.87       197\n",
      "\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 29635.1790 - accuracy: 0.8681\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 46800.4427 - accuracy: 0.7833\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 29648.9134 - accuracy: 0.8250\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 31249.0247 - accuracy: 0.8146\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 37732.9557 - accuracy: 0.7389\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29437.9141 - accuracy: 0.7181\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 29740.0664 - accuracy: 0.8146\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 26385.0879 - accuracy: 0.8146\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 23415.9850 - accuracy: 0.8146\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 17794.7107 - accuracy: 0.8250\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 28422.1836 - accuracy: 0.7938\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21748.1309 - accuracy: 0.8042\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 24829.4180 - accuracy: 0.8042\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 16490.8945 - accuracy: 0.8146\n",
      "Epoch 15/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 19982.9264 - accuracy: 0.8146\n",
      "1/1 - 0s - loss: 763.4229 - accuracy: 0.9375\n",
      "[[8 1]\n",
      " [0 7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94         9\n",
      "           1       0.88      1.00      0.93         7\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.94      0.94      0.94        16\n",
      "weighted avg       0.95      0.94      0.94        16\n",
      "\n",
      "Epoch 1/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 2.6922 - accuracy: 0.5087\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6940 - accuracy: 0.8309\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.5537 - accuracy: 0.9622\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.4140 - accuracy: 0.9711\n",
      "Epoch 5/15\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.9694\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.2344 - accuracy: 0.9540\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 0s 668us/step - loss: 0.1991 - accuracy: 0.9575\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.1636 - accuracy: 0.9581\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.1460 - accuracy: 0.9604\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.1337 - accuracy: 0.9618\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.1153 - accuracy: 0.9715\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.1094 - accuracy: 0.9646\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 0s 666us/step - loss: 0.0795 - accuracy: 0.9835\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.0922 - accuracy: 0.9747\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 0s 667us/step - loss: 0.0625 - accuracy: 0.9874\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x00000292FB0C6790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "7/7 - 0s - loss: 0.0542 - accuracy: 0.9848\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000292C77C8AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[108   0]\n",
      " [  3  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       108\n",
      "           1       1.00      0.97      0.98        89\n",
      "\n",
      "    accuracy                           0.98       197\n",
      "   macro avg       0.99      0.98      0.98       197\n",
      "weighted avg       0.99      0.98      0.98       197\n",
      "\n",
      "Epoch 1/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 48026.0977 - accuracy: 0.4993\n",
      "Epoch 2/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 36700.5951 - accuracy: 0.4993\n",
      "Epoch 3/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 46361.3724 - accuracy: 0.4785\n",
      "Epoch 4/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 29939.0195 - accuracy: 0.5097\n",
      "Epoch 5/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 31745.9258 - accuracy: 0.5201\n",
      "Epoch 6/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 26989.8639 - accuracy: 0.4681\n",
      "Epoch 7/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 28318.6263 - accuracy: 0.4785\n",
      "Epoch 8/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 30645.7409 - accuracy: 0.4681\n",
      "Epoch 9/15\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 26737.3867 - accuracy: 0.4993\n",
      "Epoch 10/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 20464.6348 - accuracy: 0.4681\n",
      "Epoch 11/15\n",
      "2/2 [==============================] - 0s 0s/step - loss: 21828.1914 - accuracy: 0.4993\n",
      "Epoch 12/15\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 18198.6263 - accuracy: 0.5201\n",
      "Epoch 13/15\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 12803.0592 - accuracy: 0.4208\n",
      "Epoch 14/15\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 9888.8973 - accuracy: 0.2507\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step - loss: 13333.4033 - accuracy: 0.2944\n",
      "1/1 - 0s - loss: 4197.5103 - accuracy: 0.3750\n",
      "[[0 9]\n",
      " [1 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.40      0.86      0.55         7\n",
      "\n",
      "    accuracy                           0.38        16\n",
      "   macro avg       0.20      0.43      0.27        16\n",
      "weighted avg       0.18      0.38      0.24        16\n",
      "\n",
      "Epoch 1/15\n",
      "14216/14216 [==============================] - 10s 667us/step - loss: 0.1226 - accuracy: 0.9582\n",
      "Epoch 2/15\n",
      "14216/14216 [==============================] - 9s 660us/step - loss: 0.0455 - accuracy: 0.9837\n",
      "Epoch 3/15\n",
      "14216/14216 [==============================] - 9s 664us/step - loss: 0.0370 - accuracy: 0.9880\n",
      "Epoch 4/15\n",
      "14216/14216 [==============================] - 9s 663us/step - loss: 0.0295 - accuracy: 0.9903\n",
      "Epoch 5/15\n",
      "14216/14216 [==============================] - 9s 663us/step - loss: 0.0225 - accuracy: 0.9923\n",
      "Epoch 6/15\n",
      "14216/14216 [==============================] - 9s 662us/step - loss: 0.0204 - accuracy: 0.9931\n",
      "Epoch 7/15\n",
      "14216/14216 [==============================] - 9s 663us/step - loss: 0.0175 - accuracy: 0.9939\n",
      "Epoch 8/15\n",
      "14216/14216 [==============================] - 9s 663us/step - loss: 0.0150 - accuracy: 0.9946\n",
      "Epoch 9/15\n",
      "14216/14216 [==============================] - 10s 675us/step - loss: 0.0138 - accuracy: 0.9952\n",
      "Epoch 10/15\n",
      "14216/14216 [==============================] - 10s 727us/step - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 11/15\n",
      "14216/14216 [==============================] - 10s 728us/step - loss: 0.0131 - accuracy: 0.9956\n",
      "Epoch 12/15\n",
      "14216/14216 [==============================] - 10s 723us/step - loss: 0.0117 - accuracy: 0.9959\n",
      "Epoch 13/15\n",
      "14216/14216 [==============================] - 10s 725us/step - loss: 0.0115 - accuracy: 0.9962\n",
      "Epoch 14/15\n",
      "14216/14216 [==============================] - 10s 730us/step - loss: 0.0110 - accuracy: 0.9965\n",
      "Epoch 15/15\n",
      "14216/14216 [==============================] - 12s 815us/step - loss: 0.0102 - accuracy: 0.9967\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_test_function.<locals>.test_function at 0x00000292C6D6A1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3554/3554 - 2s - loss: 0.0079 - accuracy: 0.9975\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000292C6D6A280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[56706   224]\n",
      " [   60 56736]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56930\n",
      "           1       1.00      1.00      1.00     56796\n",
      "\n",
      "    accuracy                           1.00    113726\n",
      "   macro avg       1.00      1.00      1.00    113726\n",
      "weighted avg       1.00      1.00      1.00    113726\n",
      "\n",
      "Epoch 1/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 89643.0589 - accuracy: 0.4981\n",
      "Epoch 2/15\n",
      "23/23 [==============================] - 0s 728us/step - loss: 36686.8621 - accuracy: 0.5456\n",
      "Epoch 3/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 5758.5149 - accuracy: 0.5378\n",
      "Epoch 4/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 2546.0889 - accuracy: 0.5522\n",
      "Epoch 5/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 1653.1213 - accuracy: 0.5544\n",
      "Epoch 6/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 847.3773 - accuracy: 0.6601\n",
      "Epoch 7/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 833.1554 - accuracy: 0.6456\n",
      "Epoch 8/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 598.9383 - accuracy: 0.6173\n",
      "Epoch 9/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 915.5363 - accuracy: 0.6362\n",
      "Epoch 10/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 705.5826 - accuracy: 0.6214\n",
      "Epoch 11/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 595.2220 - accuracy: 0.6359\n",
      "Epoch 12/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 688.8026 - accuracy: 0.6676\n",
      "Epoch 13/15\n",
      "23/23 [==============================] - 0s 729us/step - loss: 414.8944 - accuracy: 0.6612\n",
      "Epoch 14/15\n",
      "23/23 [==============================] - 0s 728us/step - loss: 337.1474 - accuracy: 0.6615\n",
      "Epoch 15/15\n",
      "23/23 [==============================] - 0s 729us/step - loss: 298.4540 - accuracy: 0.6609\n",
      "6/6 - 0s - loss: 213.6039 - accuracy: 0.5944\n",
      "[[73 10]\n",
      " [63 34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.88      0.67        83\n",
      "           1       0.77      0.35      0.48        97\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.65      0.62      0.57       180\n",
      "weighted avg       0.66      0.59      0.57       180\n",
      "\n",
      "Epoch 1/15\n",
      "14214/14214 [==============================] - 11s 708us/step - loss: 0.1606 - accuracy: 0.9427\n",
      "Epoch 2/15\n",
      "14214/14214 [==============================] - 10s 714us/step - loss: 0.0391 - accuracy: 0.9883\n",
      "Epoch 3/15\n",
      "14214/14214 [==============================] - 10s 713us/step - loss: 0.0291 - accuracy: 0.9913\n",
      "Epoch 4/15\n",
      "14214/14214 [==============================] - 11s 740us/step - loss: 0.0235 - accuracy: 0.9929\n",
      "Epoch 5/15\n",
      "14214/14214 [==============================] - 11s 755us/step - loss: 0.0193 - accuracy: 0.9940\n",
      "Epoch 6/15\n",
      "14214/14214 [==============================] - 11s 748us/step - loss: 0.0184 - accuracy: 0.9949\n",
      "Epoch 7/15\n",
      "14214/14214 [==============================] - 11s 751us/step - loss: 0.0172 - accuracy: 0.9948\n",
      "Epoch 8/15\n",
      "14214/14214 [==============================] - 11s 750us/step - loss: 0.0162 - accuracy: 0.9955\n",
      "Epoch 9/15\n",
      "14214/14214 [==============================] - 11s 755us/step - loss: 0.0166 - accuracy: 0.9950\n",
      "Epoch 10/15\n",
      "14214/14214 [==============================] - 11s 740us/step - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 11/15\n",
      "14214/14214 [==============================] - 11s 751us/step - loss: 0.0128 - accuracy: 0.9963\n",
      "Epoch 12/15\n",
      "14214/14214 [==============================] - 11s 745us/step - loss: 0.0133 - accuracy: 0.9963\n",
      "Epoch 13/15\n",
      "14214/14214 [==============================] - 11s 748us/step - loss: 0.0128 - accuracy: 0.9964\n",
      "Epoch 14/15\n",
      "14214/14214 [==============================] - 11s 747us/step - loss: 0.0119 - accuracy: 0.9965\n",
      "Epoch 15/15\n",
      "14214/14214 [==============================] - 11s 751us/step - loss: 0.0109 - accuracy: 0.9971\n",
      "3554/3554 - 2s - loss: 0.0110 - accuracy: 0.9970\n",
      "[[56535   238]\n",
      " [  104 56834]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56773\n",
      "           1       1.00      1.00      1.00     56938\n",
      "\n",
      "    accuracy                           1.00    113711\n",
      "   macro avg       1.00      1.00      1.00    113711\n",
      "weighted avg       1.00      1.00      1.00    113711\n",
      "\n",
      "Epoch 1/15\n",
      "23/23 [==============================] - 0s 728us/step - loss: 13514.5451 - accuracy: 0.4932\n",
      "Epoch 2/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 3470.6853 - accuracy: 0.4710\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 727us/step - loss: 487.1259 - accuracy: 0.5526\n",
      "Epoch 4/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 255.6368 - accuracy: 0.5731\n",
      "Epoch 5/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 270.7318 - accuracy: 0.5720\n",
      "Epoch 6/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 165.4094 - accuracy: 0.5991\n",
      "Epoch 7/15\n",
      "23/23 [==============================] - 0s 364us/step - loss: 74.0208 - accuracy: 0.5849\n",
      "Epoch 8/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 30.5039 - accuracy: 0.6114\n",
      "Epoch 9/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 73.7084 - accuracy: 0.5664\n",
      "Epoch 10/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 48.9909 - accuracy: 0.5965\n",
      "Epoch 11/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 138.5072 - accuracy: 0.5650\n",
      "Epoch 12/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 167.4579 - accuracy: 0.5794\n",
      "Epoch 13/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 87.1735 - accuracy: 0.6222\n",
      "Epoch 14/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 56.0869 - accuracy: 0.5882\n",
      "Epoch 15/15\n",
      "23/23 [==============================] - 0s 727us/step - loss: 37.2061 - accuracy: 0.6123\n",
      "6/6 - 0s - loss: 137.5689 - accuracy: 0.5674\n",
      "[[18 77]\n",
      " [ 0 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.19      0.32        95\n",
      "           1       0.52      1.00      0.68        83\n",
      "\n",
      "    accuracy                           0.57       178\n",
      "   macro avg       0.76      0.59      0.50       178\n",
      "weighted avg       0.78      0.57      0.49       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(all_data_x)):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(all_data_x[i], all_data_y[i], test_size = 0.2)\n",
    "    model = Sequential([\n",
    "        Dense(20, activation = 'relu'),\n",
    "        Dense(5, activation = 'relu'),\n",
    "        Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=15)\n",
    "    test_loss, test_acc = model.evaluate(x_test,y_test,verbose = 2)\n",
    "    y_pred = model.predict(x_test)\n",
    "    preds_index = np.argmax(y_pred, axis = 1)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y_test,preds_index))\n",
    "    print(classification_report(y_test, preds_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
